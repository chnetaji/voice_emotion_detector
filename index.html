<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Voice Emotion Detection</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <script>
      let model;

      // Load a pre-trained TensorFlow model for emotion detection
      async function loadModel() {
        model = await tf.loadLayersModel(
          "path-to-your-pretrained-model/model.json"
        );
      }

      async function detectEmotion(audioBufferData) {
        // Convert audio buffer data into a format suitable for the model
        const audioTensor = tf.tensor(audioBufferData.getChannelData(0)); // Use a single channel

        // Preprocess audio if necessary (normalize, reshape, etc.)
        const input = audioTensor.reshape([1, audioTensor.shape[0], 1]); // Reshape for the model input

        const predictions = await model.predict(input);
        const emotion = getEmotionFromPredictions(predictions);
        emotionResult.textContent = `Emotion: ${emotion}`;
      }

      // Map model predictions to emotion labels
      function getEmotionFromPredictions(predictions) {
        const emotionLabels = ["happy", "sad", "angry", "neutral"]; // Example emotions
        const predictedIndex = predictions.argMax(-1).dataSync()[0];
        return emotionLabels[predictedIndex];
      }

      loadModel(); // Call when the page loads to initialize the model
    </script>
  </head>
  <body>
    <h1>Voice Emotion Detection</h1>
    <button id="start">Start Recording</button>
    <button id="stop" disabled>Stop Recording</button>
    <p id="emotion-result">Emotion: N/A</p>

    <script>
      let audioContext;
      let mediaRecorder;
      let audioChunks = [];

      const startBtn = document.getElementById("start");
      const stopBtn = document.getElementById("stop");
      const emotionResult = document.getElementById("emotion-result");

      // Get user's microphone and start recording
      async function startRecording() {
        const stream = await navigator.mediaDevices.getUserMedia({
          audio: true,
        });
        audioContext = new (window.AudioContext || window.webkitAudioContext)();
        mediaRecorder = new MediaRecorder(stream);

        mediaRecorder.ondataavailable = (event) => {
          audioChunks.push(event.data);
        };

        mediaRecorder.start();
        startBtn.disabled = true;
        stopBtn.disabled = false;
      }

      function stopRecording() {
        mediaRecorder.stop();
        startBtn.disabled = false;
        stopBtn.disabled = true;

        mediaRecorder.onstop = async () => {
          const audioBlob = new Blob(audioChunks, { type: "audio/wav" });
          const audioBuffer = await audioBlob.arrayBuffer();
          const audioBufferData = await audioContext.decodeAudioData(
            audioBuffer
          );

          // Process audioBufferData to detect emotion
          detectEmotion(audioBufferData);
          audioChunks = [];
        };
      }

      startBtn.addEventListener("click", startRecording);
      stopBtn.addEventListener("click", stopRecording);
    </script>
  </body>
</html>
